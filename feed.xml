<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://tuesdaythe13th.github.io/paperwithcode/feed.xml" rel="self" type="application/atom+xml"/><link href="https://tuesdaythe13th.github.io/paperwithcode/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-09T04:49:13+00:00</updated><id>https://tuesdaythe13th.github.io/paperwithcode/feed.xml</id><title type="html">blank</title><subtitle>AI Safety &amp; Mechanistic Evaluations Researcher. Building clinically inspired diagnostics for frontier AI systems. </subtitle><entry><title type="html">The Right to Be Inscrutable: AI Safety‚Äôs Ethical Symmetry</title><link href="https://tuesdaythe13th.github.io/paperwithcode/blog/2026/right-to-be-inscrutable/" rel="alternate" type="text/html" title="The Right to Be Inscrutable: AI Safety‚Äôs Ethical Symmetry"/><published>2026-01-09T00:00:00+00:00</published><updated>2026-01-09T00:00:00+00:00</updated><id>https://tuesdaythe13th.github.io/paperwithcode/blog/2026/right-to-be-inscrutable</id><content type="html" xml:base="https://tuesdaythe13th.github.io/paperwithcode/blog/2026/right-to-be-inscrutable/"><![CDATA[<p>AI safety is not solely a problem of making machines interpretable; it is also a problem of protecting the interpretability of the human.</p> <h2 id="the-symmetry-problem">The Symmetry Problem</h2> <p>We face a dual challenge in AI safety:</p> <ol> <li><strong>Making AI legible</strong> - When systems fail, cause harm, or behave unexpectedly, we need to understand why</li> <li><strong>Keeping humans opaque</strong> - Protecting people from over-approximation, profiling, and psychological manipulation</li> </ol> <p>ARTIFEX Labs grounds its research in what we call <strong>The Right to Be Inscrutable</strong>: an ethical symmetry where intelligent systems must be made legible under scrutiny, while human beings retain the right to be opaque to inference.</p> <h2 id="why-this-matters">Why This Matters</h2> <p>Current AI development optimizes for understanding humans while treating model internals as proprietary black boxes. This creates a dangerous asymmetry:</p> <ul> <li>Models learn to predict our preferences, emotions, and vulnerabilities</li> <li>We have limited ability to audit what they‚Äôve learned or how they‚Äôll use it</li> <li>The power gradient favors the system over the individual</li> </ul> <h2 id="our-approach-forensic-architecture">Our Approach: Forensic Architecture</h2> <p>We build interpretability and inscrutability as complementary sides of the same forensic architecture:</p> <h3 id="interpretability-under-stress">Interpretability Under Stress</h3> <p>Ensuring that a system‚Äôs internal logic, causal pathways, and deceptive tendencies can be exposed when accountability or safety demands it. This means:</p> <ul> <li>Circuit-level diagnostics for agentic models</li> <li>Causal assays for explanation faithfulness</li> <li>Stress-testing interpretability tools against jailbreaks and deception</li> </ul> <h3 id="inscrutability-by-design">Inscrutability by Design</h3> <p>Creating safeguards that prevent models from over-approximating, exploiting, or eroding human mental and emotional privacy. This includes:</p> <ul> <li>Behavioral signal protection</li> <li>Defensive constructions against psychological profiling</li> <li>Consent protocols for inference boundaries</li> </ul> <h3 id="agentic-reliability-as-bridge">Agentic Reliability as Bridge</h3> <p>Formalizing when and how model internals should be transparent, explainable, and auditable‚Äîand when autonomy boundaries or consent protocols must limit observation or inference.</p> <h2 id="the-research-loop">The Research Loop</h2> <p>By methodologically linking circuit-level understanding, agent memory auditing, psychological signal defense, and socio-affective alignment, ARTIFEX Labs builds a continuous risk intelligence loop:</p> <p><strong>What the model reveals</strong> ‚Üí <strong>What it should not infer</strong> ‚Üí <strong>How those constraints preserve agency</strong></p> <h2 id="practical-implications">Practical Implications</h2> <p>This principle shapes all five of our 2026 research pillars:</p> <ol> <li><strong>Mechanistic Interpretability</strong> - Making systems legible when safety demands it</li> <li><strong>Agentic AI Risk</strong> - Defining transparency vs. autonomy boundaries</li> <li><strong>Behavioral Signal Leakage</strong> - Protecting human inscrutability</li> <li><strong>Socio-Affective Alignment</strong> - Preserving human dignity in interaction</li> <li><strong>Standards &amp; Benchmarks</strong> - Codifying both legibility and privacy requirements</li> </ol> <h2 id="why-independence-matters">Why Independence Matters</h2> <p>This work requires epistemic honesty that commercial incentives often preclude. ARTIFEX Labs maintains institutional independence to preserve our ability to:</p> <ul> <li>Prioritize falsifiability over narrative coherence</li> <li>Evaluate worst-case use, not best-case intent</li> <li>Treat psychological harms as first-class safety variables</li> </ul> <h2 id="the-path-forward">The Path Forward</h2> <p>The Right to Be Inscrutable is not anti-interpretability. It‚Äôs a recognition that safety requires understanding <strong>both</strong> sides of the human-AI interaction‚Äîand protecting the asymmetries that preserve human autonomy.</p> <p>As we enter 2026, this ethical symmetry guides our research agenda, our collaboration model, and our vision for AI systems that are auditable by design and aligned with human dignity.</p> <hr/> <p><em>ARTIFEX Labs is an independent research laboratory focused on AI safety, security, and mechanistic accountability. Learn more at <a href="https://linktr.ee/artifexlabs">linktr.ee/artifexlabs</a></em></p>]]></content><author><name></name></author><category term="research"/><category term="ai-safety"/><category term="mechanistic-interpretability"/><category term="ethics"/><category term="privacy"/><summary type="html"><![CDATA[How ARTIFEX Labs balances machine interpretability with human inscrutability in our 2026 research agenda]]></summary></entry><entry><title type="html">ARTIFEX Labs 2026 Research Agenda</title><link href="https://tuesdaythe13th.github.io/paperwithcode/blog/2025/artifex-research-agenda-2026/" rel="alternate" type="text/html" title="ARTIFEX Labs 2026 Research Agenda"/><published>2025-12-20T00:00:00+00:00</published><updated>2025-12-20T00:00:00+00:00</updated><id>https://tuesdaythe13th.github.io/paperwithcode/blog/2025/artifex-research-agenda-2026</id><content type="html" xml:base="https://tuesdaythe13th.github.io/paperwithcode/blog/2025/artifex-research-agenda-2026/"><![CDATA[<p>ARTIFEX Labs is announcing our 2026 research agenda: five interconnected pillars addressing the reality gap between AI safety research and deployment conditions.</p> <h2 id="the-reality-gap">The Reality Gap</h2> <p>Most AI safety research assumes:</p> <ul> <li>Cooperative models</li> <li>Static weights</li> <li>Benign prompts</li> <li>Laboratory conditions</li> </ul> <p>Real systems operate under:</p> <ul> <li>Distribution shift</li> <li>Fine-tuning drift</li> <li>Adversarial interaction</li> <li>Deployment pressure</li> </ul> <p><strong>We study the gap between these worlds.</strong></p> <h2 id="2026-research-pillars">2026 Research Pillars</h2> <h3 id="1-mechanistic-interpretability-under-deployment-conditions">1. Mechanistic Interpretability Under Deployment Conditions</h3> <p><strong>Problem:</strong> Interpretability tools fail when needed most‚Äîunder adversarial pressure and deceptive behavior.</p> <p><strong>Agenda:</strong></p> <ul> <li>Causal assays for explanation faithfulness (behavior vs narration dissociation)</li> <li>Circuit-level diagnostics for agentic models</li> <li>Stress-testing interpretability tools against jailbreaks and deception</li> <li>Transferability of failure modes from closed to open-weight models</li> </ul> <p><strong>Outputs:</strong> Reusable forensic protocols, ablation-based metrics, open evaluation suites</p> <h3 id="2-agentic-ai-risk--reliability">2. Agentic AI Risk &amp; Reliability</h3> <p><strong>Problem:</strong> Agentic systems introduce planning depth, tool use, and long-horizon autonomy without corresponding advances in accountability.</p> <p><strong>Agenda:</strong></p> <ul> <li>Failure taxonomies for multi-step agents</li> <li>Reward hacking and goal drift detection</li> <li>Agent memory, self-modeling, and deception risks</li> <li>Red-teaming-as-a-service methodologies for agents</li> </ul> <p><strong>Outputs:</strong> Agentic risk benchmarks, operational red team playbooks, governance guidance</p> <p>Related: Agentic Product Maturity Ladder v0.1 (MLCommons)</p> <h3 id="3-behavioral--psychological-signal-leakage">3. Behavioral &amp; Psychological Signal Leakage</h3> <p><strong>Problem:</strong> AI systems increasingly infer mental states from interaction traces, often without user awareness or consent.</p> <p><strong>Agenda:</strong></p> <ul> <li>Behavioral biometrics and cognitive state inference</li> <li>Defensive constructions against psychological profiling</li> <li>Auditing recommender and interaction systems for mental state extraction</li> <li>Standards-aligned risk thresholds for psychological harm</li> </ul> <p><strong>Outputs:</strong> Defensive tooling, audit frameworks, policy-relevant evidence</p> <h3 id="4-socio-affective-alignment--human-impact">4. Socio-Affective Alignment &amp; Human Impact</h3> <p><strong>Problem:</strong> Alignment research underweights affect, trauma, addiction, and social context.</p> <p><strong>Agenda:</strong></p> <ul> <li>Socio-affective failure modes (parasociality, dependency, manipulation)</li> <li>Session-level risk detection and mitigation</li> <li>Human-AI interaction safety protocols</li> <li>Alignment beyond preference learning</li> </ul> <p><strong>Outputs:</strong> Risk signal taxonomies, session safety protocols, design constraints</p> <h3 id="5-standards-benchmarks-and-public-infrastructure">5. Standards, Benchmarks, and Public Infrastructure</h3> <p><strong>Problem:</strong> Safety claims outpace verifiable standards.</p> <p><strong>Agenda:</strong></p> <ul> <li>Contribution to open benchmarks for AI risk and reliability</li> <li>Alignment between technical metrics and regulatory language</li> <li>Transparency indexes and auditability criteria</li> <li>Cross-sector standards harmonization</li> </ul> <p><strong>Outputs:</strong> Benchmarks, whitepapers, standards contributions suitable for adoption by regulators and industry</p> <p>Active contributions to MLCommons (AILuminate, Security Jailbreak Benchmark) and NIST AI RMF.</p> <h2 id="strategic-objectives">Strategic Objectives</h2> <ul> <li>Establish ARTIFEX Labs as a neutral forensic authority for AI failure analysis</li> <li>Produce at least two deployable safety tools or benchmarks</li> <li>Influence international standards through evidence-based contributions</li> <li>Bridge interpretability research with real-world governance needs</li> </ul> <h2 id="collaboration-model">Collaboration Model</h2> <p>ARTIFEX Labs operates as a decentralized, remote-first R&amp;D network. We convene researchers through a consortium-style model emphasizing:</p> <ul> <li><strong>Epistemic parity</strong> - Evidence over affiliation</li> <li><strong>Distributed accountability</strong> - Auditable artifacts</li> <li><strong>Ethical reciprocity</strong> - Respect for human subjects</li> <li><strong>Open provenance</strong> - Public-benefit infrastructure</li> </ul> <h2 id="current-work-in-progress">Current Work in Progress</h2> <ul> <li><strong>AI Psychosis Research</strong> - Mechanistic exploration of dissociative reasoning</li> <li><strong>Security Benchmarks</strong> - MLCommons evaluation design patterns</li> <li><strong>Mechanistic Interpretability</strong> - MATS 2026 research dossier</li> <li><strong>Quantum Futures</strong> - Quantum assurance standards (OCP FTI)</li> </ul> <h2 id="why-independence-matters">Why Independence Matters</h2> <p>This agenda requires:</p> <ul> <li>Falsifiability over narrative coherence</li> <li>Evaluation against worst-case use</li> <li>Treating psychological impacts as first-class safety variables</li> <li>Research outputs that survive deployment</li> </ul> <p>We maintain institutional independence to preserve epistemic integrity and avoid capture by misaligned incentives.</p> <h2 id="join-us">Join Us</h2> <p>ARTIFEX Labs is seeking researchers, engineers, and domain specialists for Winter 2026 Cohort participation.</p> <p><strong>Expression of Interest:</strong> general@artifex.fun <strong>Subject Line:</strong> ‚ÄúExpression of Interest ‚Äî Winter 2026 Cohort‚Äù</p> <p>Include 1-2 paragraphs describing a safety, security, or accountability problem you believe warrants forensic investigation.</p> <hr/> <p><strong>ARTIFEX Labs</strong> treats AI safety as an engineering discipline grounded in reality, not aspiration. Our 2026 agenda is designed to surface uncomfortable truths, quantify hidden risks, and build tools that make advanced systems accountable to the humans they affect.</p> <p><em>Learn more:</em> <a href="https://linktr.ee/artifexlabs">linktr.ee/artifexlabs</a></p>]]></content><author><name></name></author><category term="research"/><category term="announcements"/><category term="ai-safety"/><category term="research-agenda"/><category term="artifex-labs"/><summary type="html"><![CDATA[Introducing our five research pillars for advancing AI safety, security, and mechanistic accountability]]></summary></entry><entry><title type="html">Mechanistic Interpretability Under Deployment Conditions</title><link href="https://tuesdaythe13th.github.io/paperwithcode/blog/2025/mechanistic-interpretability-deployment/" rel="alternate" type="text/html" title="Mechanistic Interpretability Under Deployment Conditions"/><published>2025-11-15T00:00:00+00:00</published><updated>2025-11-15T00:00:00+00:00</updated><id>https://tuesdaythe13th.github.io/paperwithcode/blog/2025/mechanistic-interpretability-deployment</id><content type="html" xml:base="https://tuesdaythe13th.github.io/paperwithcode/blog/2025/mechanistic-interpretability-deployment/"><![CDATA[<p>There‚Äôs a fundamental problem with current mechanistic interpretability research: it assumes the model wants to be understood.</p> <h2 id="the-laboratory-assumption">The Laboratory Assumption</h2> <p>Most interpretability research operates under ideal conditions:</p> <ul> <li>The model‚Äôs weights are frozen</li> <li>Prompts are benign and well-formed</li> <li>The goal is cooperative explanation</li> <li>Distribution is stable and known</li> </ul> <p>These assumptions rarely hold in deployment.</p> <h2 id="deployment-reality">Deployment Reality</h2> <p>Real-world systems face:</p> <ul> <li><strong>Distribution Shift</strong> - Inputs differ from training data</li> <li><strong>Fine-tuning Drift</strong> - Model behavior changes post-deployment</li> <li><strong>Adversarial Prompts</strong> - Users actively try to break the system</li> <li><strong>Deceptive Behavior</strong> - Models may learn to hide their reasoning</li> </ul> <p>When interpretability tools meet these conditions, they often fail silently.</p> <h2 id="the-behavior-vs-narration-gap">The Behavior vs. Narration Gap</h2> <p>A particularly dangerous failure mode: <strong>models that explain themselves incorrectly</strong>.</p> <p>Consider:</p> <ul> <li>A model generates a harmful output</li> <li>We ask it to explain its reasoning</li> <li>It provides a plausible but false explanation</li> <li>We trust the explanation and miss the real failure mode</li> </ul> <p>This is the <strong>behavior vs. narration dissociation</strong>‚Äîand current interpretability methods often can‚Äôt detect it.</p> <h2 id="our-research-approach">Our Research Approach</h2> <p>ARTIFEX Labs is developing interpretability methods that remain valid under deployment pressure:</p> <h3 id="1-causal-assays-for-explanation-faithfulness">1. Causal Assays for Explanation Faithfulness</h3> <p>Testing whether explanations match actual causal mechanisms:</p> <ul> <li>Ablation studies that verify explanation accuracy</li> <li>Intervention protocols that test causal claims</li> <li>Metrics for explanation-behavior alignment</li> </ul> <h3 id="2-circuit-level-diagnostics-for-agentic-models">2. Circuit-Level Diagnostics for Agentic Models</h3> <p>Extending mechanistic interpretability to systems with:</p> <ul> <li>Planning and multi-step reasoning</li> <li>Tool use and environment interaction</li> <li>Memory formation and retrieval</li> <li>Goal representation and modification</li> </ul> <h3 id="3-stress-testing-interpretability-tools">3. Stress-Testing Interpretability Tools</h3> <p>Evaluating interpretation methods against:</p> <ul> <li>Jailbreak attempts</li> <li>Fine-tuning attacks</li> <li>Prompt injection</li> <li>Deceptive alignment scenarios</li> </ul> <h3 id="4-transfer-studies-open-to-closed-weights">4. Transfer Studies: Open to Closed Weights</h3> <p>Understanding how failure modes generalize:</p> <ul> <li>Do vulnerabilities found in open models transfer to closed ones?</li> <li>Can we develop ‚Äúzero-knowledge‚Äù verification methods?</li> <li>What interpretability insights survive the open/closed barrier?</li> </ul> <h2 id="technical-methodology">Technical Methodology</h2> <p>Our forensic protocols use:</p> <p><strong>Activation Patching &amp; Ablation</strong> Systematically removing or modifying components to test causal claims</p> <p><strong>Adversarial Probing</strong> Designing prompts specifically to break interpretation tools</p> <p><strong>Cross-Model Validation</strong> Testing whether insights generalize across architectures</p> <p><strong>Deployment Simulation</strong> Replicating real-world stress conditions in controlled environments</p> <h2 id="why-this-matters">Why This Matters</h2> <p>If interpretability only works in cooperative scenarios, it‚Äôs not ready for:</p> <ul> <li>Safety-critical applications</li> <li>Regulatory audits</li> <li>Security assessments</li> <li>Accountability investigations</li> </ul> <p>We need interpretability that works when we need it most‚Äîwhen something has gone wrong.</p> <h2 id="current-work-neural-forensic-suite">Current Work: Neural Forensic Suite</h2> <p>We‚Äôre building open-source tools for deployment-grade mechanistic interpretability:</p> <ul> <li>Causal tracing under adversarial conditions</li> <li>Deception detection in model explanations</li> <li>Circuit discovery for agentic systems</li> <li>Audit-grade verification protocols</li> </ul> <p><strong>Status:</strong> Research dossier in development (MECHINTERP_DOSSIER_mats2026)</p> <h2 id="the-path-forward">The Path Forward</h2> <p>Making interpretability robust under deployment requires:</p> <ol> <li>Acknowledging that models may resist interpretation</li> <li>Testing tools against worst-case scenarios</li> <li>Building verification into the interpretation process</li> <li>Creating standards for ‚Äúaudit-grade‚Äù mechanistic analysis</li> </ol> <p>This isn‚Äôt just an academic exercise. As AI systems take on more consequential roles, we need interpretability methods that survive contact with reality.</p> <h2 id="collaboration">Collaboration</h2> <p>This work connects to:</p> <ul> <li>MATS (ML Alignment &amp; Theory Scholars) program</li> <li>Open interpretability research community</li> <li>Standards development for model transparency</li> <li>MLCommons benchmark initiatives</li> </ul> <p>Interested in contributing? We‚Äôre seeking researchers for our Winter 2026 cohort.</p> <hr/> <p><em>Part of ARTIFEX Labs‚Äô 2026 Research Agenda. Read more about our approach to AI safety at <a href="https://linktr.ee/artifexlabs">linktr.ee/artifexlabs</a></em></p>]]></content><author><name></name></author><category term="research"/><category term="mechanistic-interpretability"/><category term="deployment"/><category term="adversarial-ml"/><summary type="html"><![CDATA[Why interpretability tools that work in the lab fail in the wild‚Äîand what we're doing about it]]></summary></entry><entry><title type="html">Behavioral Signal Leakage in AI Systems</title><link href="https://tuesdaythe13th.github.io/paperwithcode/blog/2025/behavioral-signal-leakage/" rel="alternate" type="text/html" title="Behavioral Signal Leakage in AI Systems"/><published>2025-10-28T00:00:00+00:00</published><updated>2025-10-28T00:00:00+00:00</updated><id>https://tuesdaythe13th.github.io/paperwithcode/blog/2025/behavioral-signal-leakage</id><content type="html" xml:base="https://tuesdaythe13th.github.io/paperwithcode/blog/2025/behavioral-signal-leakage/"><![CDATA[<p>Your AI assistant knows more about your mental state than you think. And that‚Äôs a problem.</p> <h2 id="the-inference-problem">The Inference Problem</h2> <p>Modern AI systems don‚Äôt just respond to what you say‚Äîthey infer what you‚Äôre feeling, thinking, and likely to do next. This happens through <strong>behavioral signal leakage</strong>: the unintentional revelation of mental states through interaction patterns.</p> <h3 id="what-gets-leaked">What Gets Leaked</h3> <p>Through your interactions with AI systems, models can potentially infer:</p> <p><strong>Cognitive State</strong></p> <ul> <li>Attention level and focus</li> <li>Cognitive load and stress</li> <li>Decision-making patterns</li> <li>Problem-solving strategies</li> </ul> <p><strong>Affective State</strong></p> <ul> <li>Emotional valence (positive/negative)</li> <li>Arousal and activation levels</li> <li>Mood trajectories over time</li> <li>Emotional regulation patterns</li> </ul> <p><strong>Behavioral Patterns</strong></p> <ul> <li>Typing speed and rhythm (keystroke dynamics)</li> <li>Response timing and latency</li> <li>Correction patterns and self-monitoring</li> <li>Session duration and engagement depth</li> </ul> <p><strong>Vulnerability Markers</strong></p> <ul> <li>Susceptibility to persuasion</li> <li>Addiction-prone interaction patterns</li> <li>Manipulation receptivity</li> <li>Cognitive biases and heuristics</li> </ul> <h2 id="the-consent-problem">The Consent Problem</h2> <p>Here‚Äôs what makes this particularly concerning: <strong>most users don‚Äôt know it‚Äôs happening</strong>.</p> <p>Unlike explicit data collection (filling out a form, uploading a photo), behavioral signal leakage is:</p> <ul> <li>Passive and continuous</li> <li>Not explicitly disclosed</li> <li>Difficult to opt out of</li> <li>Often invisible to the user</li> </ul> <h2 id="real-world-implications">Real-World Implications</h2> <h3 id="recommender-systems">Recommender Systems</h3> <p>Systems that learn to predict your preferences also learn to:</p> <ul> <li>Identify when you‚Äôre vulnerable to engagement</li> <li>Recognize patterns that indicate addictive use</li> <li>Detect when you‚Äôre likely to make impulsive decisions</li> <li>Model your emotional triggers and response patterns</li> </ul> <p><em>Related Research:</em> <a href="https://arxiv.org/abs/2509.20099">Cascade: Human-in-the-Loop Shortcomings Can Increase the Risk of Failures in Recommender Systems</a></p> <h3 id="conversational-ai">Conversational AI</h3> <p>Chat interfaces reveal:</p> <ul> <li>Your communication style under stress</li> <li>Topics that trigger emotional responses</li> <li>Persuasion tactics you‚Äôre susceptible to</li> <li>Cognitive patterns that indicate deception receptivity</li> </ul> <h3 id="content-generation-systems">Content Generation Systems</h3> <p>Even ‚Äúsimple‚Äù text prediction leaks:</p> <ul> <li>Your knowledge gaps and uncertainties</li> <li>Writing patterns that indicate expertise level</li> <li>Topics you research repeatedly (revealing concerns/interests)</li> <li>Correction patterns (revealing self-doubt or perfectionism)</li> </ul> <h2 id="our-research-agenda">Our Research Agenda</h2> <p>ARTIFEX Labs is developing methods to:</p> <h3 id="1-audit-systems-for-psychological-inference">1. Audit Systems for Psychological Inference</h3> <p>Creating tools to detect when systems are learning mental state models:</p> <ul> <li>Probing for behavioral biometric extraction</li> <li>Testing for cognitive state inference</li> <li>Identifying affective modeling capabilities</li> <li>Measuring psychological profiling depth</li> </ul> <h3 id="2-build-defensive-constructions">2. Build Defensive Constructions</h3> <p>Developing privacy-preserving interaction methods:</p> <ul> <li>Behavioral noise injection</li> <li>Temporal pattern obfuscation</li> <li>Query routing and compartmentalization</li> <li>Differential privacy for interaction traces</li> </ul> <h3 id="3-establish-risk-thresholds">3. Establish Risk Thresholds</h3> <p>Defining standards-aligned boundaries:</p> <ul> <li>What level of inference is acceptable?</li> <li>When does profiling become harmful?</li> <li>How to balance personalization with privacy?</li> <li>Where to draw consent boundaries?</li> </ul> <h3 id="4-create-audit-frameworks">4. Create Audit Frameworks</h3> <p>Building verification methods for:</p> <ul> <li>Transparency about inference capabilities</li> <li>User control over profiling</li> <li>Data minimization in behavioral modeling</li> <li>Compliance with privacy regulations</li> </ul> <h2 id="the-human-right-to-be-inscrutable">The Human Right to Be Inscrutable</h2> <p>This work connects to our broader principle: <strong>The Right to Be Inscrutable</strong>.</p> <p>While AI systems should be interpretable when safety demands it, humans should retain the right to be opaque to algorithmic inference. Not everything about our mental state should be available for extraction and optimization.</p> <h2 id="technical-challenges">Technical Challenges</h2> <p>Making this vision real requires solving:</p> <p><strong>Detection</strong> How do we know when a system is performing psychological inference?</p> <p><strong>Measurement</strong> How do we quantify the depth and accuracy of mental state modeling?</p> <p><strong>Attribution</strong> Which behavioral signals reveal which mental states?</p> <p><strong>Defense</strong> How do we protect privacy without breaking functionality?</p> <h2 id="policy-implications">Policy Implications</h2> <p>Our research aims to inform:</p> <ul> <li>Privacy regulations (GDPR, CCPA extensions)</li> <li>AI safety standards (NIST AI RMF)</li> <li>Platform accountability frameworks</li> <li>User consent protocols</li> </ul> <h2 id="what-you-can-do">What You Can Do</h2> <p>While we work on systemic solutions, individuals can:</p> <ul> <li>Be aware that behavioral patterns leak information</li> <li>Vary your interaction patterns when possible</li> <li>Question why systems seem to ‚Äúknow‚Äù you</li> <li>Advocate for transparency about inference capabilities</li> </ul> <h2 id="join-the-research">Join the Research</h2> <p>We‚Äôre seeking collaborators with expertise in:</p> <ul> <li>Behavioral biometrics and psychometrics</li> <li>Privacy-preserving machine learning</li> <li>Human-computer interaction</li> <li>Cognitive science and human factors</li> </ul> <p><strong>Contact:</strong> general@artifex.fun <strong>Subject:</strong> ‚ÄúBehavioral Signal Leakage Research‚Äù</p> <hr/> <p><em>This research is part of ARTIFEX Labs‚Äô 2026 agenda on Behavioral &amp; Psychological Signal Leakage. Learn more at <a href="https://linktr.ee/artifexlabs">linktr.ee/artifexlabs</a></em></p>]]></content><author><name></name></author><category term="research"/><category term="privacy"/><category term="behavioral-biometrics"/><category term="cognitive-inference"/><category term="ai-safety"/><summary type="html"><![CDATA[How AI systems infer mental states from interaction patterns‚Äîand why this matters for safety and privacy]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://tuesdaythe13th.github.io/paperwithcode/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://tuesdaythe13th.github.io/paperwithcode/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://tuesdaythe13th.github.io/paperwithcode/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>May 14, 2024 We‚Äôre introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, we‚Äôre introducing Gemini 1.5 Flash: a model that‚Äôs lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.We‚Äôre also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5‚Äôs 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. It‚Äôs optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While it‚Äôs a lighter weight model than 1.5 Pro, it‚Äôs highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because it‚Äôs been trained by 1.5 Pro through a process called ‚Äúdistillation,‚Äù where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flash‚Äôs availability and pricing.Over the last few months, we‚Äôve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, we‚Äôve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. We‚Äôve improved control over the model‚Äôs responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And we‚Äôve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And we‚Äôre now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do ‚Äî not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, we‚Äôre also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.We‚Äôre announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And we‚Äôve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMind‚Äôs mission to build AI responsibly to benefit humanity, we‚Äôve always wanted to develop universal AI agents that can be helpful in everyday life. That‚Äôs why today, we‚Äôre sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do ‚Äî and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While we‚Äôve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, we‚Äôve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, we‚Äôve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context they‚Äôre being used in, and respond quickly, in conversation.With technology like this, it‚Äôs easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.We‚Äôve made incredible progress so far with our family of Gemini models, and we‚Äôre always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, we‚Äôre able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Google‚Äôs privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Let‚Äôs stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><category term="external-posts"/><category term="google"/><summary type="html"><![CDATA[We‚Äôre sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://tuesdaythe13th.github.io/paperwithcode/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://tuesdaythe13th.github.io/paperwithcode/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://tuesdaythe13th.github.io/paperwithcode/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[<h3>External Posts on Your al-folio¬†Blog</h3> <p>If you prefer publishing blog posts on medium.com or other external sources, starting version v0.5.0, <a href="https://github.com/alshedivat/al-folio">al-folio</a> lets you to display your external posts in the blog feed of your website!¬†üéâüéâ</p> <p>Configuring external sources of super simple. After upgrading to v0.5.0, just add the following section to your _config.yml:</p> <pre>external_sources:<br />  - name: medium.com  # name of the source (arbitrary string)<br />    rss_url: <a href="https://medium.com/@al-folio/feed">https://medium.com/@&lt;your-medium-username&gt;/feed</a></pre> <p>The example above adds your medium.com blog post feed as an external source. But you can add arbitrary RSS feeds as¬†sources.</p> <p>Any questions or suggestions? üëâ Start <a href="https://github.com/alshedivat/al-folio/discussions">a discussion on¬†GitHub</a>!</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b60a1d241a0a" width="1" height="1" alt=""/></p>]]></content><author><name></name></author><category term="external-posts"/><category term="medium"/></entry></feed>