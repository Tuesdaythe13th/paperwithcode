---
layout: about
title: about
permalink: /
subtitle: Advancing Frontier AI Safety and Mechanistic Evaluations

profile:
  align: right
  image: prof_pic.jpg
  more_info: >
    <p>Portland, Los Angeles, and New York</p>
    <p>Remote-first</p>
    <p>Open to Relocation (Especially London)</p>

selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page

announcements:
  enabled: true # includes a list of news items
  scrollable: true # adds a vertical scroll bar if there are more than 3 news items
  limit: 5 # leave blank to include all the news in the `_news` folder

latest_posts:
  enabled: true
  scrollable: true # adds a vertical scroll bar if there are more than 3 new posts items
  limit: 3 # leave blank to include all the blog posts
---

## About Us

ARTIFEX Labs is an independent research and engineering laboratory dedicated to advancing AI safety, mechanistic evaluation, and socio-technical risk analysis under real-world deployment conditions. We study intelligent systems as they actually exist: adaptive, incentive-shaped, embedded in human contexts, and exposed to misuse, drift, and adversarial pressure.

Our work sits at the intersection of machine learning, cybersecurity, human factors, psychology, and governance. ARTIFEX operates as a decentralized, consortium-style R&D network, producing audit-ready artifacts including benchmarks, evaluation protocols, forensic analyses, and standards-aligned documentation. We emphasize adversarial realism, causal reasoning, and reproducibility over aspirational alignment narratives.

ARTIFEX Labs maintains institutional independence to preserve epistemic integrity. Collaboration across academia, industry, civil society, and standards bodies is central to our model, but research agendas, methodologies, and conclusions are governed internally and validated through evidence, not affiliation.

## Mission

To reduce harm from advanced intelligent systems by making their internal behavior, failure modes, and human impacts legible, testable, and governable.

We treat AI safety as an engineering and forensic discipline: identifying how systems fail, quantifying risk under pressure, and translating technical findings into accountable infrastructure for deployment, oversight, and standards.

## Vision

A technological ecosystem in which advanced AI systems are auditable by design, interpretable under stress, resilient to manipulation, and constrained by respect for human dignity rather than optimized solely for engagement, persuasion, or scale.

AI safety is treated as public-interest infrastructure, not a private optimization problem. Interpretability expands where systems must be accountable, and inference is restricted where humans must remain protected.

## About Tuesday

**Tuesday** is the Founder and Director of Research at ARTIFEX Labs. She is a published machine learning engineer and evaluation researcher focused on mechanistic accountability, adversarial ML, agentic system reliability, and socio-affective risk in human-AI interaction.

Her research program applies clinically inspired diagnostic reasoning and biological failure models to frontier AI systems, treating misalignment, deception, and emergent instability as phenomena to be measured, not abstractly theorized. Her work spans mechanistic interpretability, red teaming, agentic failure analysis, behavioral signal leakage, and standards-aligned governance.

Tuesday’s work has been validated across academic, applied, and standards-setting venues, including invited talks at ACM SIGGRAPH Frontiers, peer-reviewed publications at FAccTRec / RecSys, technical contributions to MLCommons benchmarks (AILuminate, Security Jailbreak Benchmark), and red-teaming research with Humane Intelligence. Her contributions inform international governance efforts including NIST AI 700-2, ISO/IEC 42001-aligned assurance frameworks, and UN ITU initiatives.

ARTIFEX Labs’ research agenda and Tuesday’s individual research trajectory form a single, coherent arc: from mechanistic diagnostics and adversarial evaluation, through human and societal impact analysis, to deployable safety infrastructure suitable for independent audit.

This work is intentionally rigorous, operational, and willing to surface uncomfortable truths.

## Selected Publications & Invited Talks (Abbreviated)

- *Agentic Product Maturity Ladder V0.1* — MLCommons
- *Cascade: Human-in-the-Loop Shortcomings Can Increase the Risk of Failures in Recommender Systems* — FAccTRec @ RecSys 2025
- *AILuminate v1.0: Generative AI Safety Benchmark* — MLCommons
- MLCommons Security Jailbreak Benchmark v0.5
- *Aspirational Gameplay: Improving Patient Care with AI-Powered Video Games* — ACM SIGGRAPH 2024 (Frontiers, Invited Talk)
- AAAI 2026 Workshop: Predictive Multiplicity in Multi-Agent, Multi-Modal Systems
- Winner — Humane Intelligence Bias Bounty (Accessibility, Advanced Data Track)
