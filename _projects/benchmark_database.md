---
layout: page
title: The Benchmark Database
description: Comprehensive catalog of AI evaluation benchmarks and safety standards
img: assets/img/benchmark_db_preview.jpg
importance: 3
category: research
related_publications: true
---

## The Benchmark Database

A comprehensive, searchable database cataloging AI evaluation benchmarks, safety standards, and assessment frameworks across the AI safety ecosystem.

## Features

- **Benchmark Catalog**: Extensive collection of evaluation frameworks from MLCommons, NIST, and research institutions
- **Taxonomy System**: Organized by risk domain, capability area, and evaluation methodology
- **Standards Mapping**: Cross-references to ISO/IEC 42001, NIST AI RMF, and other governance frameworks
- **Research Integration**: Links to academic publications and technical reports

## Coverage Areas

- Agentic AI evaluation (ARES, AILuminate Agentic)
- Security and jailbreak testing
- Multimodal system assessment
- Bias and fairness benchmarks
- Reliability and safety metrics

## Related Work

Supports evaluation design patterns presented to MLCommons Leadership and contributes to:
- AILuminate v1.0 benchmark development
- MLCommons Security Jailbreak Benchmark v0.5
- NIST AI 700-2 standards

[View Database Preview](https://www.canva.com/design/DAGZQkMw4z8/preview) *(design mockup)*
